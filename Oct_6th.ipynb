{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normal\n",
    "    - Tension_Normal.csv\n",
    "    - NTRNL_Normal # subfolder\n",
    "        - 05-31 23_35_19.txt\n",
    "        ...\n",
    "    - XTRNL_Normal # subfolder\n",
    "        - 20230531_233519.csv\n",
    "        ...\n",
    "- CoreExposed\n",
    "    - Tension_CoreExposed.csv\n",
    "    - NTRNL_CoreExposed # subfolder\n",
    "        - 06-01 16_04_36.txt\n",
    "        ...\n",
    "    - XTRNL_CoreExposed # subfolder\n",
    "        - 20230601_160436.csv\n",
    "        ...\n",
    "- CoreSevered\n",
    "    - Tension_CoreSevered.csv\n",
    "    - NTRNL_CoreSevered # subfolder\n",
    "        - 06-01 01_49_17.txt\n",
    "        ...\n",
    "    - XTRNL_CoreSevered # subfolder\n",
    "        - 20230601_014917.csv\n",
    "        ...  \n",
    "- SideDamaged\n",
    "    - Tension_SideDamaged.csv\n",
    "    - NTRNL_SideDamaged # subfolder\n",
    "        - 05-31 20_54_14.txt\n",
    "        ...\n",
    "    - XTRNL_SideDamaged # subfolder\n",
    "        - 20230531_205414.csv\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import scipy.stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance\n",
    "import warnings\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 수집한 신호로부터 대푯값 추출 및 표준화\n",
    "##### 변수 'Tension', 'Mean', 'Variance', 'Skewness', 'Kurtosis', 'Type', 'ConfucionMatrix' 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Directory_Path = 'C:/Users/99kit/Desktop/CapstoneDesign2/'\n",
    "list_root_directory = ['Normal', 'CoreExposed']#, 'SideDamaged']#, 'CoreSevered', 'SideDamaged']\n",
    "\n",
    "def calculate_statistics(data):\n",
    "    mean = np.mean(data)\n",
    "    var = np.var(data)\n",
    "    skew = scipy.stats.skew(data)\n",
    "    kurt = scipy.stats.kurtosis(data)\n",
    "    return mean, var, skew, kurt\n",
    "\n",
    "data_frames = {}\n",
    "\n",
    "for directory in list_root_directory:\n",
    "    os.chdir(os.path.join(Directory_Path, directory))\n",
    "\n",
    "    Tension = pd.read_csv(f\"Tension_{directory}.csv\", encoding='cp949')\n",
    "    Tension['Unnamed: 0'] = pd.to_datetime(Tension['Unnamed: 0'], format='[%Y/%m/%d] %H:%M:%S')\n",
    "    Tension['Formatted_Time'] = Tension['Unnamed: 0'].dt.strftime(f'NTRNL_{directory}\\\\%m-%d %H_%M_%S.txt')\n",
    "\n",
    "    list_max_values = []\n",
    "    for csv_file in glob.glob('*/*.csv'):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        list_max_values.append(df['External Sound'].max())\n",
    "    Tension['Max_EXT_Sound'] = list_max_values\n",
    "\n",
    "    statistics = {}\n",
    "    for txt_file in glob.glob('*/*.txt'):\n",
    "        with open(txt_file, 'r') as file:\n",
    "            data = [int(line.strip().split(',')[0]) for line in file if len(line.strip().split(',')) == 3]\n",
    "            statistics[txt_file] = calculate_statistics(data)\n",
    "\n",
    "    for stat_name in ['Mean', 'Variance', 'Skewness', 'Kurtosis']:\n",
    "        Tension[stat_name] = Tension['Formatted_Time'].map(lambda x: statistics.get(x, (np.nan, np.nan, np.nan, np.nan))[['Mean', 'Variance', 'Skewness', 'Kurtosis'].index(stat_name)])\n",
    "        Tension[stat_name].interpolate(inplace=True)\n",
    "\n",
    "    Q1 = Tension['Max_EXT_Sound'].quantile(0.25)\n",
    "    Q3 = Tension['Max_EXT_Sound'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers_max_ext_sound = Tension[(Tension['Max_EXT_Sound'] < lower_bound) | (Tension['Max_EXT_Sound'] > upper_bound)]\n",
    "\n",
    "    Q1_mean = Tension['Mean'].quantile(0.25)\n",
    "    Q3_mean = Tension['Mean'].quantile(0.75)\n",
    "    IQR_mean = Q3_mean - Q1_mean\n",
    "    lower_bound_mean = Q1_mean - 1.5 * IQR_mean\n",
    "    upper_bound_mean = Q3_mean + 1.5 * IQR_mean\n",
    "    outliers_mean_ntrnl_sound = Tension[(Tension['Mean'] < lower_bound_mean) | (Tension['Mean'] > upper_bound_mean)]\n",
    "\n",
    "    common_indices = outliers_max_ext_sound.index.intersection(outliers_mean_ntrnl_sound.index)\n",
    "    Tension.loc[common_indices, 'Mean'] = np.nan\n",
    "    Tension['Mean'].interpolate(inplace=True)\n",
    "\n",
    "    Tension = Tension.drop(['Formatted_Time', 'Max_EXT_Sound'], axis=1)\n",
    "    Tension['Type'] = directory\n",
    "\n",
    "    Tension.loc[Tension['Type'] != 'Normal', 'Confusion Matrix'] = 'Others' # 이전에 Anomaly로 표현함\n",
    "    Tension.loc[Tension['Type'] == 'Normal', 'Confusion Matrix'] = 'Normal'\n",
    "\n",
    "    data_frames[directory] = Tension\n",
    "\n",
    "# tb_data = pd.concat(data_frames.values(), ignore_index=True)\n",
    "# tb_data = tb_data.rename(columns={'벨트처짐': 'Tension'})\n",
    "\n",
    "# list_independent_variables = ['Tension', 'Mean', 'Variance', 'Skewness', 'Kurtosis']\n",
    "\n",
    "# data_standardized = StandardScaler().fit_transform(tb_data[list_independent_variables])\n",
    "# tb_data[list_independent_variables] = pd.DataFrame(data_standardized, columns=list_independent_variables)\n",
    "\n",
    "# tb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 코드 전체 재실행이 너무 오래 걸려서 따로 똑 떼어냄\n",
    "\n",
    "tb_data = pd.concat(data_frames.values(), ignore_index=True)\n",
    "tb_data = tb_data.rename(columns={'벨트처짐': 'Tension'})\n",
    "\n",
    "list_independent_variables = ['Tension', 'Mean', 'Variance', 'Skewness', 'Kurtosis']\n",
    "\n",
    "data_standardized = StandardScaler().fit_transform(tb_data[list_independent_variables])\n",
    "tb_data[list_independent_variables] = pd.DataFrame(data_standardized, columns=list_independent_variables)\n",
    "\n",
    "tb_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mahalanobis 거리를 이용한 관리한계 설정을 위해 Normal 데이터의 다변량(근데 왜 Multiple이 아니라 Multivariate임?) 정규성 검정...은 나중에 제대로 해봐야겠다.\n",
    "##### Normal 데이터가 각 차원에서 여러 군집으로 나뉘지 않는 다는 것만 확인하면 될 것 같다.\n",
    "##### 4가지 Type의 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 극단 이상치 제거한 바이올린 플롯 그리기\n",
    "plt.figure(figsize=(12, 8))  # 그림 크기 설정\n",
    "\n",
    "# 각 열에 대해 이상치 제거한 박스 플롯 그리기\n",
    "for i, col in enumerate(list_independent_variables, 1):\n",
    "    plt.subplot(2, 3, i)  # 2x3 그리드에서 i번째 위치에 서브플롯 생성\n",
    "\n",
    "    # 이상치 제거 (IQR 기반)\n",
    "    Q1 = tb_data[col].quantile(0.25)\n",
    "    Q3 = tb_data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    data_no_outliers = tb_data[(tb_data[col] >= lower_bound) & (tb_data[col] <= upper_bound)]\n",
    "\n",
    "    sns.violinplot(x= 'Type', y = col, data=data_no_outliers)\n",
    "    # plt.title(f'{col} (Outliers Removed) by Type')  # 서브플롯 제목 설정\n",
    "    plt.xlabel('')#Type')  # x축 레이블 설정\n",
    "    plt.ylabel(col)  # y축 레이블 설정\n",
    "    plt.xticks(rotation=22)  # x축 레이블 회전\n",
    "\n",
    "plt.tight_layout()  # 레이아웃 조정\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3차원 plot\n",
    "# 독립 변수와 타겟 변수를 선택\n",
    "X = tb_data[list_independent_variables]  # 독립 변수\n",
    "y = tb_data['Type']  # 타겟 변수\n",
    "\n",
    "# 상호 정보량 계산\n",
    "mutual_info = mutual_info_classif(X, y, discrete_features='auto', random_state=42)\n",
    "\n",
    "# 결과 출력\n",
    "importance_scores = pd.Series(mutual_info, index=X.columns)\n",
    "importance_scores = importance_scores.sort_values(ascending=False)\n",
    "print('변수 중요도')\n",
    "print(importance_scores)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "tension = tb_data['Tension']\n",
    "variance = tb_data['Variance']\n",
    "mean = tb_data['Mean']\n",
    "type = tb_data['Type']\n",
    "\n",
    "for t in tb_data['Type'].unique().tolist():\n",
    "    mask = type == t\n",
    "    ax.scatter(tension[mask], variance[mask], mean[mask], label = t, marker = '.')\n",
    "\n",
    "ax.set_xlabel('Tension', rotation=45)\n",
    "ax.set_ylabel('Variance', rotation=45)\n",
    "ax.set_zlabel('Mean', rotation=45)\n",
    "\n",
    "azimuth_angle = -90\n",
    "elevation_angle = 0\n",
    "ax.view_init(elev=elevation_angle, azim=azimuth_angle)\n",
    "\n",
    "ax.legend()\n",
    "plt.title('Scatter Plot of Tension, Variance, and Mean by Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 극단 이상치 제거하지 않은 바이올린 플롯\n",
    "plt.figure(figsize=(12, 8))  # 그림 크기 설정\n",
    "\n",
    "# 각 열에 대해 이상치 제거한 박스 플롯 그리기\n",
    "for i, col in enumerate(list_independent_variables, 1):\n",
    "    plt.subplot(2, 3, i)  # 2x3 그리드에서 i번째 위치에 서브플롯 생성\n",
    "\n",
    "    sns.violinplot(x='Confusion Matrix', y=col, data=tb_data)\n",
    "    plt.title(f'{col} by Type')  # 서브플롯 제목 설정\n",
    "    plt.xlabel('Type')  # x축 레이블 설정\n",
    "    plt.ylabel(col)  # y축 레이블 설정\n",
    "    plt.xticks(rotation=45)  # x축 레이블 회전\n",
    "\n",
    "plt.tight_layout()  # 레이아웃 조정\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normal를 분산이 가장 큰 축들에 사영하여 시각화했다. 동글동글한가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal 데이터의 PCA 3차원 시각화\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "pca = PCA(n_components = 3)\n",
    "pca_normal = pca.fit_transform(tb_data.loc[tb_data['Type'] == 'Normal', list_independent_variables])\n",
    "list_pca_normal_cols = [f'PC{i+1}' for i in range(pca_normal.shape[1])]\n",
    "pca_normal = pd.DataFrame(data = pca_normal, columns = list_pca_normal_cols)\n",
    "\n",
    "pc1 = pca_normal['PC1']\n",
    "pc2 = pca_normal['PC2']\n",
    "pc3 = pca_normal['PC3']\n",
    "\n",
    "ax.scatter(pc1, pc2, pc3, marker = 'o')\n",
    "\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "\n",
    "azimuth_angle = 50\n",
    "elevation_angle = 40\n",
    "ax.view_init(elev=elevation_angle, azim=azimuth_angle)\n",
    "\n",
    "# plt.title('PCA Scatter Plot of Normal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normal에 대해 집단과 데이터 포인트 사이의 Mahalanobis 거리 계산\n",
    "##### 신뢰수준(n%)을 정하고 여러 개의 n분위수를 Bootstrap으로 구한 뒤 그 평균을 관리한계로 설정\n",
    "##### False Alarm 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### numpy로 거리 계산이 안 됨. T²가 e-13으로 계산되고, 거리 계산 위해 루트 씌우면 NaN 나옴.\n",
    "##### 왜일까. 암튼 그래서 scipy.spatial.distance.mahalanobis 썼다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14255개 데이터의 마할라노비스 거리 계산\n",
    "warnings.filterwarnings('ignore') # 모든 경고 메시지 무시\n",
    "\n",
    "type_normal = tb_data[tb_data['Type'] == 'Normal']\n",
    "type_not_normal = tb_data[tb_data['Type'] != 'Normal']\n",
    "\n",
    "normal_mean = type_normal[list_independent_variables].mean()\n",
    "normal_cov = type_normal[list_independent_variables].cov()\n",
    "\n",
    "def mahalanobis_distance(x):\n",
    "    return distance.mahalanobis(x, normal_mean, normal_cov)\n",
    "\n",
    "type_normal['Mahalanobis Distance'] = type_normal[list_independent_variables].apply(mahalanobis_distance, axis = 1)\n",
    "type_not_normal['Mahalanobis Distance'] = type_not_normal[list_independent_variables].apply(mahalanobis_distance, axis = 1)\n",
    "\n",
    "# bootstrap_iterations = 500\n",
    "# confidence_level = 95\n",
    "\n",
    "# list_bootstrap_statistics = []\n",
    "\n",
    "# for _ in range(bootstrap_iterations):\n",
    "#     bootstrap_sample = np.random.choice(type_normal['Mahalanobis Distance'], size = len(type_normal), replace = True)\n",
    "#     list_bootstrap_statistics.append(np.percentile(bootstrap_sample, confidence_level))\n",
    "\n",
    "# control_limit = np.mean(list_bootstrap_statistics)\n",
    "# print(f'confidence level: {confidence_level}%')\n",
    "# print(f'control limit: {control_limit}')\n",
    "\n",
    "# type_normal.loc[type_normal['Mahalanobis Distance'] >= control_limit, 'Confusion Matrix'] = 'FalseAlarm' # 'Normal' = TrueNegative\n",
    "# type_not_normal.loc[type_not_normal['Mahalanobis Distance'] >= control_limit, 'Confusion Matrix'] = 'TrueAlarm'\n",
    "# type_not_normal.loc[type_not_normal['Mahalanobis Distance'] < control_limit, 'Confusion Matrix'] = 'Type2Error' # 'Anomaly' = TruePositive\n",
    "\n",
    "# tb_data = pd.concat([type_normal, type_not_normal], axis = 0)\n",
    "\n",
    "# tb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bootstrap_iterations = 100\n",
    "list_ER = []\n",
    "\n",
    "for confidence_level in range(1, 101):\n",
    "    list_bootstrap_statistics = [] # 반복 횟수(100)만큼의 통계량을 저장할 빈 리스트\n",
    "    \n",
    "    for _ in range(bootstrap_iterations): # list_bootstrap_statistics에 100개의 통계량이 저장됨. 이들은 모두 같은 신뢰구간에서 추출된 통계량임.\n",
    "        bootstrap_sample = np.random.choice(type_normal['Mahalanobis Distance'], size = len(type_normal), replace = True) # Normal 데이터(3581)개와 같은 양의 표본을 복원추출\n",
    "        list_bootstrap_statistics.append(np.percentile(bootstrap_sample, confidence_level)) # 확보한 3581개 표본에서 n(=confidence_level)분위수 한 개 추출. list_bootstrap_statistics에 추가\n",
    "\n",
    "    control_limit = np.mean(list_bootstrap_statistics) # 100개 통계량의 평균\n",
    "\n",
    "    type_normal['Confusion Matrix'] = np.where(type_normal['Mahalanobis Distance'] >= control_limit, 'FalseAlarm', 'TrueNegative')\n",
    "    type_not_normal['Confusion Matrix'] = np.where(type_not_normal['Mahalanobis Distance'] >= control_limit, 'TrueAlarm', 'Type2Error')\n",
    "\n",
    "    ftb_data = pd.concat([type_normal, type_not_normal], axis = 0) # 관리한계 안 넘은 정상들은 애초에 Normal로 레이블링 되어 있었다.\n",
    "    \n",
    "    Type1ErrorRates = round(len(ftb_data[ftb_data['Confusion Matrix'] == 'FalseAlarm'])/len(tb_data[tb_data['Type'] == 'Normal']), 4) # 제1종 오류 비율\n",
    "    Type2ErrorRates = round(len(ftb_data[ftb_data['Confusion Matrix'] == 'Type2Error'])/len(tb_data[tb_data['Type'] != 'Normal']), 4) # 제2종 오류 비율\n",
    "    \n",
    "    list_ER.append([Type1ErrorRates, Type2ErrorRates])\n",
    "    \n",
    "x = [item[0] for item in list_ER]\n",
    "y = [item[1] for item in list_ER]\n",
    "\n",
    "plt.plot([0, 1], [1, 0], linestyle='--', color='red')#, label='y = -x')\n",
    "plt.plot(x, y, marker='.', linestyle='-')\n",
    "\n",
    "plt.xlabel(\"Actual Type Ⅰ Error Rates\")\n",
    "plt.ylabel(\"Actual Type Ⅱ Error Rates\")\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_slope = []\n",
    "\n",
    "for i in range(1, len(x)):\n",
    "    delta_x = x[i] - x[i - 1]\n",
    "    delta_y = y[i] - y[i - 1]\n",
    "    rate = delta_y / delta_x\n",
    "    list_slope.append(rate)\n",
    "\n",
    "plt.plot(range(1, len(x)), list_slope, marker='.', linestyle='-')\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Slope (Rate of Change)')\n",
    "# plt.title('Slope Visualization')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_slope[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ER[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore') # 모든 경고 메시지 무시\n",
    "\n",
    "# type_normal = tb_data[tb_data['Type'] == 'Normal']\n",
    "# type_not_normal = tb_data[tb_data['Type'] != 'Normal']\n",
    "\n",
    "normal_mean = type_normal[list_independent_variables].mean()\n",
    "normal_cov = type_normal[list_independent_variables].cov()\n",
    "\n",
    "def mahalanobis_distance(x):\n",
    "    return distance.mahalanobis(x, normal_mean, normal_cov)\n",
    "\n",
    "# type_normal['Mahalanobis Distance'] = type_normal[list_independent_variables].apply(mahalanobis_distance, axis = 1)\n",
    "# type_not_normal['Mahalanobis Distance'] = type_not_normal[list_independent_variables].apply(mahalanobis_distance, axis = 1)\n",
    "\n",
    "type_normal = tb_data[tb_data['Type'] == 'Normal']\n",
    "type_not_normal = tb_data[tb_data['Type'] != 'Normal']\n",
    "\n",
    "type_normal['Mahalanobis Distance'] = type_normal[list_independent_variables].apply(mahalanobis_distance, axis = 1)\n",
    "type_not_normal['Mahalanobis Distance'] = type_not_normal[list_independent_variables].apply(mahalanobis_distance, axis = 1)\n",
    "\n",
    "bootstrap_iterations = 500\n",
    "confidence_level = 70\n",
    "\n",
    "list_bootstrap_statistics = []\n",
    "\n",
    "for _ in range(bootstrap_iterations):\n",
    "    bootstrap_sample = np.random.choice(type_normal['Mahalanobis Distance'], size = len(type_normal), replace = True)\n",
    "    list_bootstrap_statistics.append(np.percentile(bootstrap_sample, confidence_level))\n",
    "\n",
    "control_limit = np.mean(list_bootstrap_statistics)\n",
    "print(f'confidence level: {confidence_level}%')\n",
    "print(f'control limit: {control_limit}')\n",
    "\n",
    "type_normal.loc[type_normal['Mahalanobis Distance'] >= control_limit, 'Confusion Matrix'] = 'FalseAlarm' # 'Normal' = TrueNegative\n",
    "type_not_normal.loc[type_not_normal['Mahalanobis Distance'] >= control_limit, 'Confusion Matrix'] = 'TrueAlarm'\n",
    "type_not_normal.loc[type_not_normal['Mahalanobis Distance'] < control_limit, 'Confusion Matrix'] = 'Type2Error' # 'Anomaly' = TruePositive\n",
    "\n",
    "ftb_data = pd.concat([type_normal, type_not_normal], axis = 0)\n",
    "\n",
    "ftb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mahalanobis_distance = ftb_data['Mahalanobis Distance']\n",
    "\n",
    "false_alarm_indices = ftb_data[ftb_data['Confusion Matrix'] == 'FalseAlarm'].index\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for t in tb_data['Type'].unique():\n",
    "    type_indices = tb_data[tb_data['Type'] == t].index\n",
    "    plt.plot(type_indices, mahalanobis_distance[type_indices], label=f'{t}') # 각 'Type'에 대한 Mahalanobis 거리 플롯\n",
    "    \n",
    "plt.axhline(y=control_limit, color='red', linestyle='--', label='Control Limit') # Control Limit 그리기\n",
    "\n",
    "plt.scatter(false_alarm_indices, mahalanobis_distance[false_alarm_indices], c='red', marker='x', label='FalseAlarm') # 'Confusion Matrix' 값이 'Type1Error'인 데이터 포인트 표시\n",
    "\n",
    "plt.ylabel('Mahalanobis Distance')\n",
    "# plt.title('Mahalanobis Distance Monitoring Chart with Type Ⅰ Error')\n",
    "plt.legend()\n",
    "\n",
    "plt.ylim(2, 8.2) # y축 범위 설정\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tb_data[list_independent_variables]  # 독립 변수\n",
    "y = tb_data['Confusion Matrix']  # 타겟 변수\n",
    "\n",
    "# 상호 정보량 계산\n",
    "mutual_info = mutual_info_classif(X, y, discrete_features='auto', random_state=42)\n",
    "\n",
    "# 결과 출력\n",
    "importance_scores = pd.Series(mutual_info, index=X.columns)\n",
    "importance_scores = importance_scores.sort_values(ascending=False)\n",
    "print('변수 중요도')\n",
    "print(importance_scores)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "variance = tb_data['Variance']\n",
    "kurtosis = tb_data['Kurtosis']\n",
    "tension = tb_data['Tension']\n",
    "mean = tb_data['Mean']\n",
    "skewness = tb_data['Skewness']\n",
    "confusion_matrix = tb_data['Confusion Matrix']\n",
    "\n",
    "for t in tb_data['Confusion Matrix'].unique().tolist():\n",
    "    mask = confusion_matrix == t\n",
    "    ax.scatter(mean[mask], skewness[mask], tension[mask], label = t, marker = '.')\n",
    "\n",
    "ax.set_xlabel('Mean', rotation=45)\n",
    "ax.set_ylabel('Skewness', rotation=45)\n",
    "ax.set_zlabel('Tension', rotation=45)\n",
    "\n",
    "azimuth_angle = 0\n",
    "elevation_angle = 90\n",
    "ax.view_init(elev=elevation_angle, azim=azimuth_angle)\n",
    "\n",
    "ax.legend()\n",
    "plt.title('Scatter Plot of Variance, Kurtosis, and Tension (Confusion Matrix)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 관리한계를 넘은 관측치(Out-of-control)는 False Alarm과 True Alarm으로 이루어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ooc = ftb_data[(ftb_data['Confusion Matrix'] == 'FalseAlarm') | (ftb_data['Confusion Matrix'] == 'TrueAlarm')]\n",
    "ooc = ooc.reset_index(drop = True) # False는 기존 인덱스를 새로운 열로 추가\n",
    "ooc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ooc[list_independent_variables]  # 독립 변수\n",
    "y = ooc['Confusion Matrix']  # 타겟 변수\n",
    "\n",
    "# 상호의존정보\n",
    "mutual_info = mutual_info_classif(X, y, discrete_features='auto', random_state=42)\n",
    "\n",
    "# 결과 출력\n",
    "importance_scores = pd.Series(mutual_info, index=X.columns)\n",
    "importance_scores = importance_scores.sort_values(ascending=False)\n",
    "print('변수 중요도')\n",
    "print(importance_scores)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "tension = ooc['Tension']\n",
    "mean = ooc['Mean']\n",
    "skewness = ooc['Skewness']\n",
    "confusion_matrix = ooc['Confusion Matrix']\n",
    "\n",
    "for t in ooc['Confusion Matrix'].unique().tolist():\n",
    "    mask = confusion_matrix == t\n",
    "    ax.scatter(tension[mask], mean[mask], skewness[mask], label = t, marker = '.')\n",
    "\n",
    "ax.set_xlabel('Tension')\n",
    "ax.set_ylabel('Mean')\n",
    "ax.set_zlabel('Skewness')\n",
    "\n",
    "azimuth_angle = 90\n",
    "elevation_angle = 180\n",
    "ax.view_init(elev=elevation_angle, azim=azimuth_angle)\n",
    "\n",
    "ax.legend()\n",
    "plt.title('Scatter Plot of Tension, Mean, and Skewness')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 하하하하하하하하하하하하하하하하하하하하하\n",
    "##### IQR을 이용하여 False Alarm의 Extreme Outliers를 정의할 수 있는 Outer Fence를 만듭시다.(Q3 + IQR * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FA = ftb_data[(ftb_data['Confusion Matrix'] == 'FalseAlarm')]\n",
    "FA = FA.reset_index(drop = True) # False는 기존 인덱스를 새로운 열로 추가\n",
    "\n",
    "Q1 = FA['Tension'].quantile(0.25)\n",
    "Q3 = FA['Tension'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outer_fence = Q3 + 3 * IQR\n",
    "print(f'Upper Outer Fence: {outer_fence}')\n",
    "\n",
    "tension = ooc['Tension']\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for t in ooc['Confusion Matrix'].unique():\n",
    "    type_indices = ooc[ooc['Confusion Matrix'] == t].index\n",
    "    plt.plot(type_indices, tension[type_indices], label=f'{t}') # 각 'Type'에 대한 Mahalanobis 거리 플롯\n",
    "    \n",
    "plt.axhline(y=outer_fence, color='red', linestyle='--', label='Outer Fence (3 * IQR)') # Control Limit 그래프 그리기\n",
    "\n",
    "plt.ylabel('Tension')\n",
    "plt.title('Tension Monitoring Chart')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fac_target = ooc[(ooc['Tension'] < outer_fence)]\n",
    "extreme_outliers = ooc[(ooc['Tension'] >= outer_fence)]\n",
    "fac_target = fac_target.reset_index(drop = False)\n",
    "fac_target # Control Limit은 넘었지만 Outer Fence는 안 넘음\n",
    "# extreme_outliers # FAC 필요 없이 아묻따 Anomaly. 바로 Type 분류하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독립 변수와 타겟 변수를 선택\n",
    "X = fac_target[list_independent_variables]  # 독립 변수\n",
    "y = fac_target['Confusion Matrix']  # 타겟 변수\n",
    "\n",
    "# 상호 정보량 계산\n",
    "mutual_info = mutual_info_classif(X, y, discrete_features='auto', random_state=42)\n",
    "\n",
    "# 결과 출력\n",
    "importance_scores = pd.Series(mutual_info, index=X.columns)\n",
    "importance_scores = importance_scores.sort_values(ascending=False)\n",
    "print('변수 중요도')\n",
    "print(importance_scores)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "skewness = fac_target['Skewness']\n",
    "variance = fac_target['Variance']\n",
    "tension = fac_target['Tension']\n",
    "confusion_matrix = fac_target['Confusion Matrix']\n",
    "\n",
    "for t in fac_target['Confusion Matrix'].unique().tolist():\n",
    "    mask = confusion_matrix == t\n",
    "    ax.scatter(skewness[mask], variance[mask], tension[mask], label = t, marker = '.')\n",
    "\n",
    "ax.set_xlabel('Skewness', rotation=45)\n",
    "ax.set_ylabel('Variance', rotation=45)\n",
    "ax.set_zlabel('Tension', rotation=45)\n",
    "\n",
    "azimuth_angle = 0\n",
    "elevation_angle = 90\n",
    "ax.view_init(elev=elevation_angle, azim=azimuth_angle)\n",
    "\n",
    "ax.legend()\n",
    "plt.title('Scatter Plot of Skewness, Variance, and Tension')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 간단하게 LDA로 분류해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y) # 비율 유지\n",
    "\n",
    "# LDA 모델 생성 및 학습\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_train_lda = lda.fit_transform(X_train, y_train)\n",
    "\n",
    "# 테스트 데이터를 LDA에 적용\n",
    "X_test_lda = lda.transform(X_test)\n",
    "\n",
    "# 분류 모델 (예: 로지스틱 회귀)을 통해 훈련 및 평가\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_lda, y_train)\n",
    "y_pred = classifier.predict(X_test_lda)\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=y_train.unique())\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "for label in y.unique():\n",
    "    if label == 'TrueAlarm':\n",
    "        marker = '.'\n",
    "    elif label == 'FalseAlarm':\n",
    "        marker = 'o'\n",
    "    plt.scatter(X_train_lda[y_train == label], [0] * sum(y_train == label), label=label, alpha=0.2, marker=marker, s=100) # 선명도alpha\n",
    "\n",
    "plt.xlabel('LDA Component 1')\n",
    "plt.ylim(-0.1, 0.1)  # 1차원 공간에 표시\n",
    "plt.legend(loc='best') #최적 위치로\n",
    "plt.title('LDA Projection of Out-of-control Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = [item[1] for item in list_ActualErrorRates]\n",
    "y_values = [item[2] for item in list_ActualErrorRates]\n",
    "\n",
    "plt.plot(x_values, y_values, marker='.', linestyle='-')\n",
    "\n",
    "plt.xlabel(\"Actual Type Ⅰ Error Rates\")\n",
    "plt.ylabel(\"Actual Type Ⅱ Error Rates\")\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ActualErrorRates = []\n",
    "bootstrap_iterations = 100 # Bootstraping 반복 횟수\n",
    "\n",
    "type_normal = tb_data[tb_data['Type'] == 'Normal']\n",
    "type_not_normal = tb_data[tb_data['Type'] != 'Normal']\n",
    "\n",
    "type_normal['Mahalanobis Distance'] = type_normal[list_independent_variables].apply(mahalanobis_distance, axis=1) # 'Normal'('Type') 데이터의 Mahalanobis 거리\n",
    "type_not_normal['Mahalanobis Distance'] = type_not_normal[list_independent_variables].apply(mahalanobis_distance, axis=1) # 'Normal'이 아닌 데이터의 거리\n",
    "\n",
    "normal_mean = type_normal[list_independent_variables].mean()\n",
    "normal_cov = type_normal[list_independent_variables].cov()\n",
    "\n",
    "def mahalanobis_distance(x): # Mahalanobis 거리 계산을 위한 함수\n",
    "    return distance.mahalanobis(x, normal_mean, normal_cov)\n",
    "\n",
    "for confidence_level in range(1, 100):\n",
    "    list_bootstrap_statistics = [] # 원하는 통계량을 저장할 빈 리스트\n",
    "\n",
    "    for _ in range(bootstrap_iterations): # 3581개 복원추출(1회)을 100회 반복. 매회 신뢰구간에 따른 임계점을 빈 리스트에 저장\n",
    "        bootstrap_sample = np.random.choice(type_normal['Mahalanobis Distance'], size=len(type_normal), replace=True) # 'Normal'에 대해 원본 데이터프레임 행 갯수만큼 복원추출\n",
    "        list_bootstrap_statistics.append(np.percentile(bootstrap_sample, confidence_level))\n",
    "\n",
    "    control_limit = np.mean(list_bootstrap_statistics) # 100개 임계점의 평균. 관리한계\n",
    "\n",
    "    type_normal['Confusion Matrix'] = np.where(type_normal['Mahalanobis Distance'] >= control_limit, 'FalseAlarm', 'TrueNegative')\n",
    "    type_not_normal['Confusion Matrix'] = np.where(type_not_normal['Mahalanobis Distance'] >= control_limit, 'TrueAlarm', 'Type2Error')\n",
    "\n",
    "    new_tb_data = pd.concat([type_normal, type_not_normal], axis=0)\n",
    "\n",
    "    ooc = new_tb_data[(new_tb_data['Confusion Matrix'] == 'FalseAlarm') | (new_tb_data['Confusion Matrix'] == 'TrueAlarm')]\n",
    "\n",
    "    FA = new_tb_data[(new_tb_data['Confusion Matrix'] == 'FalseAlarm')]\n",
    "    \n",
    "    Q1 = FA['Tension'].quantile(0.25)\n",
    "    Q3 = FA['Tension'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outer_fence = Q3 + 3 * IQR\n",
    "\n",
    "    fac_target = ooc[(ooc['Tension'] < outer_fence)]\n",
    "    extreme_outliers = ooc[(ooc['Tension'] >= outer_fence)]\n",
    "\n",
    "    X = fac_target[list_independent_variables]  # 독립변수\n",
    "    y = fac_target['Confusion Matrix']  # 종속변수\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # 비율 유지\n",
    "\n",
    "    n_train_normal = len(fac_target[fac_target.loc[fac_target['Confusion Matrix'].isin(y_train), 'Type'] == 'Normal'])\n",
    "    n_train_others = len(fac_target[fac_target.loc[fac_target['Confusion Matrix'].isin(y_train), 'Type'] != 'Normal'])\n",
    "\n",
    "    # LDA 모델 생성 및 학습\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    X_train_lda = lda.fit_transform(X_train, y_train)\n",
    "\n",
    "    X_test_lda = lda.transform(X_test) # 테스트 데이터를 LDA에 적용\n",
    "\n",
    "    classifier = LogisticRegression() # LogisticRegression으로 구성된 분류기\n",
    "    classifier.fit(X_train_lda, y_train)\n",
    "    y_pred = classifier.predict(X_test_lda)\n",
    "\n",
    "    n_normal = len(tb_data[tb_data['Type'] == 'Normal']) # 'Normal' 데이터 개수\n",
    "    n_fault = len(tb_data[tb_data['Type'] != 'Normal']) # 'Normal'이 아닌 데이터 개수\n",
    "    n_alpha = len(y_pred[y_pred == 'FalseAlarm']) # 제1종오류 개수\n",
    "    n_beta = len(new_tb_data[new_tb_data['Confusion Matrix'] == 'Type2Error']) # 제2종오류 개수\n",
    "\n",
    "    ActualType1ErrorRates = round(n_alpha/(n_normal-n_train_normal), 4)\n",
    "    ActualType2ErrorRates = round(n_beta/(n_fault-n_train_others), 4)\n",
    "\n",
    "    list_ActualErrorRates.append([(100 - confidence_level)/100, ActualType1ErrorRates, ActualType2ErrorRates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alpha/(n_normal-n_train_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(fac_target[list_independent_variables], fac_target['Confusion Matrix'], test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "n_train_normal = len(fac_target[fac_target.loc[fac_target['Confusion Matrix'].isin(y_train), 'Type'] == 'Normal'])\n",
    "n_train_others = len(fac_target[fac_target.loc[fac_target['Confusion Matrix'].isin(y_train), 'Type'] != 'Normal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 이제 유의수준에 따른 두 가지 Type Error Rates 비교해야돼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report # 왜 가끔 말을 안 듣지\n",
    "\n",
    "list_dependent_variables = ['Type', 'Confusion Matrix']\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components = 3)\n",
    "pca_fa = pca.fit_transform(fac_target.loc[fac_target['Confusion Matrix'] == 'FalseAlarm', list_independent_variables])\n",
    "list_pca_fa_cols = [f'PC{i+1}' for i in range(pca_fa.shape[1])]\n",
    "pca_fa = pd.concat([pd.DataFrame(data = pca_fa, columns = list_pca_fa_cols), fac_target.loc[fac_target['Confusion Matrix'] == 'FalseAlarm', list_dependent_variables]], axis = 1)\n",
    "\n",
    "pca_ta = pca.transform(fac_target.loc[fac_target['Confusion Matrix'] != 'FalseAlarm', list_independent_variables])\n",
    "list_pca_ta_cols = [f'PC{i+1}' for i in range(pca_ta.shape[1])]\n",
    "pca_ta = pd.concat([pd.DataFrame(data = pca_ta, columns = list_pca_ta_cols), fac_target.loc[fac_target['Confusion Matrix'] != 'FalseAlarm', list_dependent_variables].reset_index(drop = True)], axis = 1)\n",
    "\n",
    "pca_fata = pd.concat([pca_fa, pca_ta], axis = 0)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "pc1 = pca_fata['PC1']\n",
    "pc2 = pca_fata['PC2']\n",
    "pc3 = pca_fata['PC3']\n",
    "type = pca_fata['Confusion Matrix']\n",
    "\n",
    "for t in pca_fata['Confusion Matrix'].unique().tolist():\n",
    "    mask = type == t\n",
    "    ax.scatter(pc1[mask], pc2[mask], pc3[mask], label = t, marker = '.')\n",
    "\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "\n",
    "azimuth_angle = 0\n",
    "elevation_angle = 90\n",
    "ax.view_init(elev=elevation_angle, azim=azimuth_angle)\n",
    "\n",
    "plt.legend()\n",
    "plt.title('PCA Scatter Plot')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "X = pca_fata[list_pca_ta_cols]\n",
    "y = pca_fata['Confusion Matrix']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Random Forest\n",
    "clf_rf = RandomForestClassifier(random_state=42, max_depth=7)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "y_pred_rf = clf_rf.predict(X_test)\n",
    "\n",
    "# Type Ⅰ Error와 Type Ⅱ Error의 비율을 시각화하여 모델을 선택하자.\n",
    "\n",
    "# class_labels = ['Others', 'FalseAlarm']\n",
    "\n",
    "# cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# plt.figure(figsize=(4, 3))\n",
    "\n",
    "# sns.heatmap(cm_rf, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.title('PCA & RF Confusion Matrix')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(\"PCA & Random Forest Classification Report:\")\n",
    "# print(classification_report(y_test, y_pred_rf, target_names=class_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
